# Automa√ß√£o com Terraform, AWS e Docker 
### Crie um cluster EKS de n√≠vel de produ√ß√£o com o Terraform

![Terraform](https://miro.medium.com/max/1400/1*FL83CEzVcducCEP80rGjbg.png)

### Arquitetura:

O Kubernetes tornou-se um dos principais no espa√ßo nativo da nuvem hoje.

√â uma √≥tima implementa√ß√£o para todas as organiza√ß√µes, grandes ou pequenas, mas √© um participante importante para quem deseja implantar aplicativos de maneira altamente escal√°vel e segura.

Dada minha experi√™ncia pessoal usando Kubernetes, [Terraform](https://www.terraform.io/) e AWS, decidi criar uma implementa√ß√£o que provisionar√° o cluster EKS usando dois grupos de n√≥s diferentes (spot e sob demanda) e tamb√©m dividir√° a cria√ß√£o e configura√ß√£o da infraestrutura em diferentes m√≥dulos .

Um m√≥dulo Terraform que cria recursos VPC e EKS na AWS. Este ser√° o m√≥dulo base.

Um m√≥dulo do Terraform que configura os componentes do Kubernetes no cluster EKS (controladores de entrada, manipuladores de termina√ß√£o pontual, DNS externo, namespaces etc.)

### Introdu√ß√£o:

Este artigo demonstra como usar o Terraform para provisionar o Amazon Elastic Kubernetes Service (EKS).

Aproveitaremos os m√≥dulos oficiais do [Terraform](https://www.terraform.io/language/modules/sources) para desenvolver alguns desses recursos seguindo as melhores pr√°ticas e evitar reinventar a roda.

O cluster ser√° criado em uma zona m√∫ltipla.

Vamos usar [Infraestrutura como C√≥digo](https://www.terraform.io/) para criar:

- Uma nova VPC com sub-redes p√∫blicas e privadas de v√°rias zonas.

- Um √∫nico Gateway NAT. Isso pode criar um √∫nico ponto de falha, pois o NAT Gateway est√° em uma AZ. Voc√™ pode alterar para garantir a disponibilidade total.

- Um cluster Kubernetes, com uma combina√ß√£o de inst√¢ncias spot e sob demanda do EC2 em execu√ß√£o em sub-redes privadas, com grupo de escalonamento autom√°tico baseado no uso m√©dio da CPU.

- Um Application Load Balancer (ALB) para aceitar chamadas HTTP p√∫blicas e encaminh√°-las para n√≥s do Kubernetes, bem como executar verifica√ß√µes de integridade para dimensionar os servi√ßos do Kubernetes, se necess√°rio.

- Um AWS Load Balancer Controller dentro do cluster, para receber e encaminhar solicita√ß√µes HTTP do mundo externo para pods do Kubernetes.

- Uma zona DNS com um certificado SSL para fornecer HTTPS para cada servi√ßo do Kubernetes. Usaremos o servi√ßo [DNS externo](https://github.com/kubernetes-sigs/external-dns) para gerenciar a zona do Kubernetes. Voc√™ pode ler mais sobre isso.

- Um aplicativo de amostra para implantar em nosso cluster, usando um Helm Chart.

### Pr√©-requisitos:

Voc√™ precisa ter algum conhecimento b√°sico sobre como trabalhar com Kubernetes e criar um cluster EKS usando um console de gerenciamento da AWS ou CLI e conhecimento b√°sico do Terraform.

Voc√™ precisar√° ter:

- Uma conta ativa da AWS. Voc√™ pode se inscrever e usar o n√≠vel gratuito oferecido pela AWS. Por favor, alguns dos recursos criados n√£o v√£o al√©m do teste gratuito.

- [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) e [AWS Vault](https://github.com/99designs/aws-vault) s√£o instalados e configurados em sua m√°quina local. Eu n√£o vou cobrir como fazer isso aqui. Se voc√™ tiver algum problema ao instalar e configurar, por favor, deixe-me saber no coment√°rio. Vou tentar te ajudar.

- Terraform CLI em sua m√°quina local. Usamos a vers√£o v1.19 neste documento, mas sinta-se √† vontade para usar vers√µes mais recentes, se desejar. Minha recomenda√ß√£o √© usar uma [docker-imagem](https://hub.docker.com/r/hashicorp/terraform) com um arquivo de composi√ß√£o do docker ou [tfenv](https://github.com/tfutils/tfenv) e simplificar a instala√ß√£o e o uso de uma vers√£o espec√≠fica.

- kubectl instalado em sua m√°quina local


üí∞ Texto explicativo do or√ßamento: criar alguns desses recursos, por exemplo, VPC, EKS e DNS, provavelmente trar√° algum custo ao seu faturamento mensal da AWS, pois alguns recursos podem ir al√©m do teste gratuito.
Portanto, esteja ciente disso antes de aplicar o Terraform! Voc√™ tamb√©m pode certificar-se de destruir imediatamente os recursos assim que terminar este tutorial.

### Provedor de configura√ß√£o com Terraform:

Ap√≥s uma breve introdu√ß√£o, vamos entrar em nossa infraestrutura como c√≥digo! Veremos trechos de configura√ß√£o do Terraform necess√°rios em cada etapa.
Voc√™ pode copi√°-los e tentar aplicar esses planos por conta pr√≥pria.

Come√ßamos criando uma pasta e abrindo a pasta em seu editor favorito.

A primeira coisa que devemos criar √© a [configura√ß√£o do provedor](https://www.terraform.io/language/providers). O Terraform conta com plugins chamados ‚Äúprovedores‚Äù para interagir com provedores de nuvem, provedores de SaaS e outras APIs.

As configura√ß√µes do Terraform devem declarar quais provedores s√£o necess√°rios para que o Terraform possa instal√°-los e us√°-los.

proveider.tf
```ssh
terraform {
  required_version = "1.1.9"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "4.11.0" # Opcional, mas recomendado em produ√ß√£o
    }
  }

}
```

‚úÖ Recomenda√ß√£o: √â uma boa ideia declarar a vers√£o do Terraform a ser usada para evitar quaisquer altera√ß√µes que possam afetar nossa infraestrutura se usarmos vers√µes mais recentes/antigas ao executar o Terraform no futuro.

‚úÖ Recomenda√ß√£o: Os provedores de recursos podem ser tratados automaticamente pelo Terraform com o comando init. No entanto, √© uma boa ideia defini-los explicitamente usando n√∫meros de vers√£o da maneira que fizemos acima para evitar altera√ß√µes de interrup√ß√£o de fonte de dados/recurso por vers√µes futuras.

‚úÖ Recomenda√ß√£o: A configura√ß√£o do backend √© a [Configura√ß√£o Parcial](https://www.terraform.io/language/settings/backends/configuration#partial-configuration). Precisamos disso configurado para que possamos ter v√°rios arquivos por ambiente (staging, development, prod) se necess√°rio. Isso nos permitir√° ter v√°rios arquivos de estado para cada espa√ßo de trabalho do Terraform:‚úÖ Recommendation: Backend configuration is Partial Configuration. We need this set up so that we can have several files per environment(staging, development, prod) if required. This will enable us to have several state files for each Terraform workspace:

backend.tfvars
```ssh
bucket               = "devops-demo.tfstate"
key                  = "infra.json"
region               = "eu-west-1"
workspace_key_prefix = "environment"
dynamodb_table       = "devops-demo.tfstate.lock"
```

‚úÖ Recomenda√ß√£o: √â aconselh√°vel bloquear o [estado](https://www.terraform.io/language/state/locking#state-locking) do seu backend para evitar que outros adquiram o bloqueio e potencialmente corrompam seu estado, especialmente ao executar isso em um pipeline CI/CD. Estamos usando o Amazon [DynamoDB](https://aws.amazon.com/dynamodb/?trk=2431813f-f7fb-4215-a32b-dc6bb102214d&sc_channel=ps&sc_campaign=acquisition&sc_medium=ACQ-P|PS-GO|Brand|Desktop|SU|Database|DynamoDB|EEM|EN|Text|Non-EU&s_kwcid=AL!4422!3!536452473269!e!!g!!aws%20dynamodb&ef_id=Cj0KCQjw2MWVBhCQARIsAIjbwoPpmxOYkOtYKyWGe7vK495lxUp9J2QS_gWIYWnmmrYQuXAg9oIoDNIaAsbnEALw_wcB:G:s&s_kwcid=AL!4422!3!536452473269!e!!g!!aws%20dynamodb) para isso.

‚ö†Ô∏èImportante: o bucket do S3 e a tabela do DynamoDB precisam existir antes de executar o comando terraform init. Eles n√£o ser√£o criados pelo Terraform se n√£o existirem na AWS. Voc√™ pode criar um bucket manualmente ou por meio de uma ferramenta CI/CD executando um comando como este:

```ssh
aws s3 mb s3://my-iac-bucket-name --region eu-west-1
```

Os nomes dos buckets devem ser exclusivos. Leia mais [aqui](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html).

Para a tabela do DynamoDB, a maneira mais r√°pida de fazer isso √© cri√°-la manualmente, mas voc√™ tamb√©m pode cri√°-la por meio da AWS CLI.

‚úÖ Recomenda√ß√£o: Evite definir credenciais da AWS em blocos de provedores. Em vez disso, poder√≠amos usar [vari√°veis](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#environment-variables) de ambiente para essa finalidade.

O Terraform os usar√° automaticamente para autenticar em APIs da AWS.

Usaremos o docker com este arquivo docker-compose para executar nossos comandos do Terraform.

docker-compose.yml
```ssh
version: '3.7'

services:
  terraform:
    image: hashicorp/terraform:1.1.9
    volumes:
      - .:/infra
    working_dir: /infra
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN}
```

Como mencionamos acima, usaremos o aws-vault para armazenar e acessar com seguran√ßa as credenciais da AWS em nosso ambiente de desenvolvimento.

‚ö†Ô∏èImportante: A vers√£o da imagem do docker usada no arquivo docker-compose deve ser a mesma vers√£o do Terraform que estamos usando na configura√ß√£o do provedor.

Como mencionamos acima, usaremos o [Terraform Workspaces](https://www.terraform.io/language/state/workspaces), por exemplo, o espa√ßo de trabalho de desenvolvimento para implanta√ß√£o no servidor de desenvolvimento.

Para inicializar cada workspace, devemos executar este comando:

```ssh
docker-compose -f docker-compose.yml run --rm terraform init -backend-config=config-backend.tfvars
docker-compose -f docker-compose.yml run --rm terraform workspace new development
```

Com este workspace, se quisermos executar comandos do Terraform no mesmo workspace ou alternar o workspace, podemos fazer isso executando este comando:

```ssh
docker-compose -f docker-compose.yaml run --rm terraform init -backend-config=config-backend.tfvars
docker-compose -f docker-compose.yml run --rm terraform workspace select development
```

At√© este ponto, estamos prontos para come√ßar a escrever nossa infraestrutura como c√≥digo üòÄ. 

### Fa√ßa uma pausa e pegue um caf√© ‚òïÔ∏è.

Vamos come√ßar a codificar nosso m√≥dulo base. 

Come√ßaremos com a cria√ß√£o da VPC

### Cria√ß√£o de componentes de rede e VPC

Vamos come√ßar criando uma nova [VPC](https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html) e todos os componentes de rede necess√°rios (sub-redes, [NAT](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html), [Elastic IP](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html) etc) para isolar nossos recursos relacionados ao EKS em um local seguro.

Para isso, usamos o [m√≥dulo terraform oficial](https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/latest) da AWS VPC. Estaremos usando a v3.14.0, que √© a vers√£o mais recente do m√≥dulo no momento em que escrevemos isso. Sinta-se √† vontade para mudar isso.

data.tf
```ssh
data "aws_availability_zones" "available_azs" {
  state = "available"
}
```

network.tf
```ssh
# Reserve Elastic IP para ser usado em nosso gateway NAT
resource "aws_eip" "nat_gw_eip" {
  vpc = true

  tags = {
    Name = "${var.cluster_name}-nat-eip"
  }
}

# Criar VPC usando o m√≥dulo oficial de VPC da AWS
# https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/latest
module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "3.14.0"

  name = "${var.name_prefix}-vpc"
  cidr = var.main_network_block
  azs  = data.aws_availability_zones.available_azs.names

  private_subnets = [
    # this loop will create a one-line list as ["10.0.0.0/20", "10.0.16.0/20", "10.0.32.0/20", ...]
    # with a length depending on how many Zones are available
    for zone_id in data.aws_availability_zones.available_azs.zone_ids :
    cidrsubnet(var.main_network_block, var.subnet_prefix_extension, tonumber(substr(zone_id, length(zone_id) - 1, 1)) - 1)
  ]

  public_subnets = [
    # this loop will create a one-line list as ["10.0.128.0/20", "10.0.144.0/20", "10.0.160.0/20", ...]
    # with a length depending on how many Zones are available
    # there is a zone Offset variable, to make sure no collisions are present with private subnet blocks
    for zone_id in data.aws_availability_zones.available_azs.zone_ids :
    cidrsubnet(var.main_network_block, var.subnet_prefix_extension, tonumber(substr(zone_id, length(zone_id) - 1, 1)) + var.zone_offset - 1)
  ]

  enable_nat_gateway     = true
   # ative o Gateway NAT √∫nico para economizar algum dinheiro. Isso pode criar um √∫nico ponto de falha, pois estamos criando um Gateway NAT em apenas uma AZ
   # sinta-se √† vontade para alterar essas op√ß√µes se precisar garantir a disponibilidade total
  single_nat_gateway     = true
  one_nat_gateway_per_az = false
  enable_dns_hostnames   = true
  reuse_nat_ips          = true
  external_nat_ip_ids    = [aws_eip.nat_gw_eip.id]

  # adicionar tags de VPC/sub-rede exigidas pelo EKS
  tags = {
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
  }
  public_subnet_tags = {
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
    "kubernetes.io/role/elb"                    = "1"
  }
  private_subnet_tags = {
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
    "kubernetes.io/role/internal-elb"           = "1"
  }
}

# Cria grupo de seguran√ßa para ser usado posteriormente pelo ALB de entrada
resource "aws_security_group" "alb" {
  name   = "${var.name_prefix}-alb"
  vpc_id = module.vpc.vpc_id

  ingress {
    description      = "http"
    from_port        = 80
    to_port          = 80
    protocol         = "tcp"
    cidr_blocks      = ["0.0.0.0/0"]
    ipv6_cidr_blocks = ["::/0"]
  }

  ingress {
    description      = "https"
    from_port        = 443
    to_port          = 443
    protocol         = "tcp"
    cidr_blocks      = ["0.0.0.0/0"]
    ipv6_cidr_blocks = ["::/0"]
  }

  egress {
    from_port        = 0
    to_port          = 0
    protocol         = "-1"
    cidr_blocks      = ["0.0.0.0/0"]
    ipv6_cidr_blocks = ["::/0"]
  }

  tags = {
    "Name" = "${var.name_prefix}-alb"
  }
}


  private_subnets = [
    for zone_id in data.aws_availability_zones.available_azs.zone_ids :
    cidrsubnet(var.main_network_block, var.subnet_prefix_extension, tonumber(substr(zone_id, length(zone_id) - 1, 1)) - 1)
  ]

  public_subnets = [
    for zone_id in data.aws_availability_zones.available_azs.zone_ids :
    cidrsubnet(var.main_network_block, var.subnet_prefix_extension, tonumber(substr(zone_id, length(zone_id) - 1, 1)) + var.zone_offset - 1)
  ]

  enable_nat_gateway     = true
  single_nat_gateway     = true
  one_nat_gateway_per_az = false
  enable_dns_hostnames   = true
  reuse_nat_ips          = true
  external_nat_ip_ids    = [aws_eip.nat_gw_elastic_ip.id]


  tags = {
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
  }
  public_subnet_tags = {
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
    "kubernetes.io/role/elb"                    = "1"
  }
  private_subnet_tags = {
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
    "kubernetes.io/role/internal-elb"           = "1"
  }
}
resource "aws_security_group" "alb" {
  name   = "${var.name_prefix}-alb"
  vpc_id = module.vpc.vpc_id

  ingress {
    description      = "http"
    from_port        = 80
    to_port          = 80
    protocol         = "tcp"
    cidr_blocks      = ["0.0.0.0/0"]
    ipv6_cidr_blocks = ["::/0"]
  }

  ingress {
    description      = "https"
    from_port        = 443
    to_port          = 443
    protocol         = "tcp"
    cidr_blocks      = ["0.0.0.0/0"]
    ipv6_cidr_blocks = ["::/0"]
  }

  egress {
    from_port        = 0
    to_port          = 0
    protocol         = "-1"
    cidr_blocks      = ["0.0.0.0/0"]
    ipv6_cidr_blocks = ["::/0"]
  }

  tags = {
    "Name" = "${var.name_prefix}-alb"
  }
}
```

Criaremos uma nova VPC com sub-redes em cada zona de disponibilidade com um √∫nico gateway NAT.

Estamos usando um √∫nico gateway NAT aqui para economizar custos, mas lembre-se de que isso pode criar um √∫nico ponto de falha. Voc√™ pode alterar as op√ß√µes se precisar garantir a disponibilidade total criando NAT em cada uma das zonas de disponibilidade.

Tamb√©m estamos adicionando algumas das tags [exigidas pelo EKS](https://docs.aws.amazon.com/eks/latest/userguide/network_reqs.html).

### Cria√ß√£o de cluster EKS

O pr√≥ximo componente que vamos criar √© um novo cluster Kubernetes.

Estamos usando o [m√≥dulo terraform oficial](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest) do AWS EKS.

Estaremos usando v18.21.0 com ~>(Verifique a [sintaxe de restri√ß√µes de vers√£o](https://www.terraform.io/language/expressions/version-constraints#version-constraint-syntax)), que √© a vers√£o mais recente do m√≥dulo no momento em que escrevemos isso. Sinta-se √† vontade para mudar isso.

eks.tf
```ssh
module "eks-cluster" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 18.20.2"

  cluster_name                    = var.cluster_name
  cluster_version                 = "1.22"
  cluster_endpoint_private_access = true
  cluster_endpoint_public_access  = true
  subnet_ids                      = module.vpc.private_subnets
  vpc_id                          = module.vpc.vpc_id
  eks_managed_node_groups         = var.eks_managed_node_groups

  node_security_group_additional_rules = {
    # Se voc√™ omitir isso, receber√° um erro interno: falha ao chamar o webhook, o servidor n√£o p√¥de encontrar o recurso solicitado
    # https://github.com/kubernetes-sigs/aws-load-balancer-controller/issues/2039#issuecomment-1099032289
    ingress_allow_access_from_control_plane = {
      type                          = "ingress"
      protocol                      = "tcp"
      from_port                     = 9443
      to_port                       = 9443
      source_cluster_security_group = true
      description = "Allow access from control plane to webhook port of AWS load balancer controller"
    }
    # permite conex√µes do grupo de seguran√ßa ALB
    ingress_allow_access_from_alb_sg = {
      type                     = "ingress"
      protocol                 = "-1"
      from_port                = 0
      to_port                  = 0
      source_security_group_id = aws_security_group.alb.id
    }
    # permite conex√µes do EKS com a internet
    egress_all = {
      protocol         = "-1"
      from_port        = 0
      to_port          = 0
      type             = "egress"
      cidr_blocks      = ["0.0.0.0/0"]
      ipv6_cidr_blocks = ["::/0"]
    }
    # permite conex√µes internas de EKS para EKS
    ingress_self_all = {
      protocol  = "-1"
      from_port = 0
      to_port   = 0
      type      = "ingress"
      self      = true
    }
  }
}

# Fun√ß√£o do IAM para AWS Load Balancer Controller e anexar ao EKS OIDC
# https://registry.terraform.io/modules/terraform-aws-modules/iam/aws/latest/submodules/iam-role-for-service-accounts-eks
module "eks_ingress_iam" {
  source  = "terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks"
  version = "~> 4.22.0"

  role_name                              = "load-balancer-controller"
  attach_load_balancer_controller_policy = true

  oidc_providers = {
    main = {
      provider_arn               = module.cluster.oidc_provider_arn
      namespace_service_accounts = ["kube-system:aws-load-balancer-controller"]
    }
  }
}

# Fun√ß√£o do IAM para DNS externo e anexar ao EKS OIDC
# https://registry.terraform.io/modules/terraform-aws-modules/iam/aws/latest/submodules/iam-role-for-service-accounts-eks
module "eks_external_dns_iam" {
  source  = "terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks"
  version = "~> 4.22.0"

  role_name                     = "external-dns"
  attach_external_dns_policy    = true
  external_dns_hosted_zone_arns = ["arn:aws:route53:::hostedzone/*"]

  oidc_providers = {
    main = {
      provider_arn               = module.cluster.oidc_provider_arn
      namespace_service_accounts = ["kube-system:external-dns"]
    }
  }
}

# Definir frota spot e pol√≠tica de escalonamento autom√°tico sob demanda
resource "aws_autoscaling_policy" "eks_autoscaling_policy" {
  count = length(var.eks_managed_node_groups)

  name                   = "${module.cluster.eks_managed_node_groups_autoscaling_group_names[count.index]}-autoscaling-policy"
  autoscaling_group_name = module.cluster.eks_managed_node_groups_autoscaling_group_names[count.index]
  policy_type            = "TargetTrackingScaling"

  target_tracking_configuration {
    predefined_metric_specification {
      predefined_metric_type = "ASGAverageCPUUtilization"
    }
    target_value = var.autoscaling_average_cpu
  }
}
```

asg.tf
```ssh
resource "aws_autoscaling_policy" "eks_autoscaling_policy" {
  count = length(var.eks_managed_node_groups)

  name                   = "${module.cluster.eks_managed_node_groups_autoscaling_group_names[count.index]}-autoscaling-policy"
  autoscaling_group_name = module.cluster.eks_managed_node_groups_autoscaling_group_names[count.index]
  policy_type            = "TargetTrackingScaling"

  target_tracking_configuration {
    predefined_metric_specification {
      predefined_metric_type = "ASGAverageCPUUtilization"
    }
    target_value = var.autoscaling_average_cpu
  }
}
```

Estamos criando o EKS Cluster que usa o EC2 Autoscaling Group for Kubernetes. O EC2 √© composto por inst√¢ncias spot e sob demanda com escalonamento autom√°tico para cima/para baixo com base no uso m√©dio da CPU.

### Definir fun√ß√µes do IAM

Precisamos definir algumas fun√ß√µes do IAM para Load Balancer Controller e DNS externo e anex√°-las ao [EKS OIDC](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-technical-overview.html) como mencionamos no in√≠cio. Estamos usando o [m√≥dulo Fun√ß√£o do IAM](https://registry.terraform.io/modules/terraform-aws-modules/iam/aws/latest/submodules/iam-role-for-service-accounts-eks) para conta de servi√ßo para fazer isso.

iam.tf
```ssh
# cria a fun√ß√£o do IAM para o AWS Load Balancer Controller e anexa ao EKS OIDC
module "eks_ingress_iam" {
  source  = "terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks"
  version = "~> 4.22.0"

  role_name                              = "load-balancer-controller"
  attach_load_balancer_controller_policy = true

  oidc_providers = {
    ex = {
      provider_arn               = module.cluster.oidc_provider_arn
      namespace_service_accounts = ["kube-system:aws-load-balancer-controller"]
    }
  }
}

# cria a fun√ß√£o do IAM para DNS externo e anexa ao EKS OIDC
module "eks_external_dns_iam" {
  source  = "terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks"
  version = "~> 4.22.0"

  role_name                     = "external-dns"
  attach_external_dns_policy    = true
  external_dns_hosted_zone_arns = ["arn:aws:route53:::hostedzone/*"]

  oidc_providers = {
    ex = {
      provider_arn               = module.cluster.oidc_provider_arn
      namespace_service_accounts = ["kube-system:external-dns"]
    }
  }
}
```

Como voc√™ pode ver, esses blocos do Terraform usam algumas vari√°veis. Precisamos definir e criar seus valores correspondentes.

variables.tf
```ssh
variable "eks_managed_node_groups" {
  type        = map(any)
  description = "Mapa de defini√ß√µes de grupos de n√≥s gerenciados do EKS para criar"
}
variable "autoscaling_average_cpu" {
  type        = number
  description = "Limite m√©dio de CPU para dimensionar automaticamente inst√¢ncias do EKS EC2."
}
variable "cluster_name" {
  type        = string
  description = "Nome do cluster EKS."
}
variable "name_prefix" {
  type        = string
  description = "Prefixo a ser usado em cada nome de objeto de infraestrutura criado na AWS."
}
variable "main_network_block" {
  type        = string
  description = "Bloco CIDR base a ser usado em nossa VPC."
}
variable "subnet_prefix_extension" {
  type        = number
  description = "Extens√£o de bits de bloco CIDR para calcular blocos CIDR de cada sub-rede."
}
variable "zone_offset" {
  type        = number
  description = "Deslocamento de extens√£o de bits de bloco CIDR para calcular sub-redes p√∫blicas, evitando colis√µes com sub-redes privadas."
}
```

Agora que temos nosso m√≥dulo base pronto, estamos prontos para criar nosso cluster EKS. Antes de podermos aplicar isso, precisamos definir alguns valores para essas vari√°veis.

base-develop,emt.tfvars
```ssh
cluster_name            = "devops-demo-eks-cluster"
iac_environment_tag     = "development"
name_prefix             = "devops-demo-development"
main_network_block      = "10.0.0.0/16"
subnet_prefix_extension = 4
zone_offset             = 8

autoscaling_average_cpu = 30
eks_managed_node_groups = {
  "devops-eks-spot" = {
    ami_type     = "AL2_x86_64"
    min_size     = 1
    max_size     = 16
    desired_size = 1
    instance_types = [
      "t3.medium",
    ]
    capacity_type = "SPOT"
    network_interfaces = [{
      delete_on_termination       = true
      associate_public_ip_address = true
    }]
  }
  "devops-eks-ondemand" = {
    ami_type     = "AL2_x86_64"
    min_size     = 1
    max_size     = 16
    desired_size = 1
    instance_types = [
      "t3.medium",
    ]
    capacity_type = "ON_DEMAND"
    network_interfaces = [{
      delete_on_termination       = true
      associate_public_ip_address = true
    }]
  }
}
```

Neste ponto, podemos organizar todas essas configura√ß√µes em um m√≥dulo e, em seguida, executar os comandos de fluxo de trabalho do Terraform.

main.tf
```ssh
module "base" {
  source = "./base/"

  cluster_name            = var.cluster_name
  name_prefix             = var.name_prefix
  main_network_block      = var.main_network_block
  subnet_prefix_extension = var.subnet_prefix_extension
  zone_offset             = var.zone_offset
  eks_managed_node_groups = var.eks_managed_node_groups
  autoscaling_average_cpu = var.autoscaling_average_cpu
}
```

A primeira coisa a fazer √© ter certeza de que estamos no workspace correto e validar nossa configura√ß√£o executando os seguintes comandos:

```ssh
docker-compose -f docker-compose.yaml run --rm terraform workspace select development
docker-compose -f docker-compose.yaml run --rm terraform validate
```

Depois disso, obtemos a sa√≠da do nosso plano executando o seguinte comando.

```ssh
docker-compose -f docker-compose.yaml run --rm terraform plan -out=development.tfplan -var-file=base-network-development.tfvars
```

Isso deve imprimir a sa√≠da do plano e nos fornecer os detalhes do que nossa configura√ß√£o fornecer√° quando aplicarmos.

Com tudo parecendo bem, podemos aplicar a sa√≠da do plano executando o seguinte comando:

```ssh
docker-compose -f docker-compose.yaml run --rm terraform apply development.tfplan
```

Feito a aplica√ß√£o, temos um novo cluster EKS na AWS. Agora que terminamos de criar o cluster, podemos prosseguir com a configura√ß√£o do cluster.

‚ö†Ô∏èImportante: Se voc√™ quiser fazer uma pausa neste momento ou n√£o quiser deixar a infraestrutura funcionando antes de passar para a pr√≥xima etapa, voc√™ pode destruir toda a infraestrutura executando os seguintes comandos:

```ssh
docker-compose -f docker-compose.yaml run --rm terraform destroy -var-file=base-network-development.tfvars
```

Dica: Se voc√™ n√£o quiser digitar yes ou confirmar toda vez que executar os comandos apply/destroy, voc√™ pode adicionar -auto-approve no final desses comandos.

### Configura√ß√£o do cluster EKS

Como mencionamos no in√≠cio, usaremos um m√≥dulo diferente para configurar o cluster. Buscaremos os dados do EKS Cluster usando a [fonte de dados](https://www.terraform.io/language/data-sources) Terraform.

Primeiro criamos o arquivo de configura√ß√£o do provedor que inclui todos os provedores necess√°rios (AWS, kubernetes, helm etc).

version.tf
```ssh
terraform {
  required_version = "1.1.9"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "4.11.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "2.11.0"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "2.5.1"
    }
    time = {
      source  = "hashicorp/time"
      version = "0.7.2"
    }
  }
}
```

eks.tf
```ssh
# Obtenha informa√ß√µes do cluster EKS para configurar provedores Kubernetes e Helm
data "aws_eks_cluster" "cluster" {
  name = var.cluster_name
}
data "aws_eks_cluster_auth" "cluster" {
  name = var.cluster_name
}

# Obtenha autentica√ß√£o EKS para poder gerenciar objetos k8s do Terraform
provider "kubernetes" {
  host                   = data.aws_eks_cluster.cluster.endpoint
  cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data)
  token                  = data.aws_eks_cluster_auth.cluster.token
}
provider "helm" {
  kubernetes {
    host                   = data.aws_eks_cluster.cluster.endpoint
    cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data)
    token                  = data.aws_eks_cluster_auth.cluster.token
  }
}

# Deploy spot termination handler
resource "helm_release" "spot_termination_handler" {
  name          = var.spot_termination_handler_chart_name
  chart         = var.spot_termination_handler_chart_name
  repository    = var.spot_termination_handler_chart_repo
  version       = var.spot_termination_handler_chart_version
  namespace     = var.spot_termination_handler_chart_namespace
  wait_for_jobs = true
}
```

Nesta configura√ß√£o, estamos fazendo duas coisas principais:

- Estamos recebendo nosso cluster EKS existente como fonte de dados. Precisamos disso para configurar [Kubernetes](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs) e [Helm](https://registry.terraform.io/providers/hashicorp/helm/latest/docs) Terraform Providers.

- Estamos implantando o Helm Chart para o [AWS Node Termination Handler](https://github.com/aws/aws-node-termination-handler) para inst√¢ncias spot do EC2, que cuida da realoca√ß√£o de objetos do Kubernetes quando as [inst√¢ncias spot s√£o interrompidas](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html).

### Acesso ao IAM

A pr√≥xima etapa √© configurar o acesso ao IAM necess√°rio para usu√°rios da AWS que entram em nosso cluster EKS usando o [ConfigMap](https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html) aws-auth.

data.tf
```ssh
data "aws_caller_identity" "current" {} # used for accesing Account ID and ARN
```

iam.tf
```ssh
# Crie mapas de usu√°rios de administradores e desenvolvedores
locals {
  admin_user_map_users = [
    for admin_user in var.admin_users :
    {
      userarn  = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:user/${admin_user}"
      username = admin_user
      groups   = ["system:masters"]
    }
  ]
  developer_user_map_users = [
    for developer_user in var.developer_users :
    {
      userarn  = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:user/${developer_user}"
      username = developer_user
      groups   = ["${var.name_prefix}-developers"]
    }
  ]
}

# Add 'mapUsers' section to 'aws-auth' configmap with Admins & Developers
resource "time_sleep" "wait" {
  create_duration = "180s"
  triggers = {
    cluster_endpoint = data.aws_eks_cluster.cluster.endpoint
  }
}
resource "kubernetes_config_map_v1_data" "aws_auth_users" {
  metadata {
    name      = "aws-auth"
    namespace = "kube-system"
  }

  data = {
    mapUsers = yamlencode(concat(local.admin_user_map_users, local.developer_user_map_users))
  }

  force = true

  depends_on = [time_sleep.wait]
}

# Crie uma fun√ß√£o de desenvolvedor usando o RBAC
resource "kubernetes_cluster_role" "iam_roles_developers" {
  metadata {
    name = "${var.name_prefix}-developers"
  }

  rule {
    api_groups = ["*"]
    resources  = ["pods", "pods/log", "deployments", "ingresses", "services"]
    verbs      = ["get", "list"]
  }

  rule {
    api_groups = ["*"]
    resources  = ["pods/exec"]
    verbs      = ["create"]
  }

  rule {
    api_groups = ["*"]
    resources  = ["pods/portforward"]
    verbs      = ["*"]
  }
}

# Vincule os usu√°rios do desenvolvedor com sua fun√ß√£o
resource "kubernetes_cluster_role_binding" "iam_roles_developers" {
  metadata {
    name = "${var.name_prefix}-developers"
  }

  role_ref {
    api_group = "rbac.authorization.k8s.io"
    kind      = "ClusterRole"
    name      = "${var.name_prefix}-developers"
  }

  dynamic "subject" {
    for_each = toset(var.developer_users)

    content {
      name      = subject.key
      kind      = "User"
      api_group = "rbac.authorization.k8s.io"
    }
  }
}
```

### Load Balancer

A pr√≥xima coisa que estamos criando √© um [Application Load Balancer](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html) (ALB) para lidar com solicita√ß√µes HTTP para nossos servi√ßos. 

Usaremos o servi√ßo AWS Load Balancer Controller implantado usando o Helm.

ingress.tf
```ssh
# Obt√©m a zona hospedada de DNS
# ATEN√á√ÉO: se voc√™ ainda n√£o possui uma Zona Route53, substitua esses dados por um novo recurso
data "aws_route53_zone" "hosted_zone" {
  name = var.dns_hosted_zone
}

# cria certificado SSL emitido pela AWS
resource "aws_acm_certificate" "eks_domain_cert" {
  domain_name               = var.dns_hosted_zone
  subject_alternative_names = ["*.${var.dns_hosted_zone}"]
  validation_method         = "DNS"

  tags = {
    Name = "${var.dns_hosted_zone}"
  }
}
resource "aws_route53_record" "domain_cert_validation" {
  for_each = {
    for cvo in aws_acm_certificate.eks_domain_cert.domain_validation_options : cvo.domain_name => {
      name   = cvo.resource_record_name
      record = cvo.resource_record_value
      type   = cvo.resource_record_type
    }
  }

  allow_overwrite = true
  name            = each.value.name
  records         = [each.value.record]
  ttl             = 60
  type            = each.value.type
  zone_id         = data.aws_route53_zone.base_domain.zone_id
}
resource "aws_acm_certificate_validation" "eks_domain_cert_validation" {
  certificate_arn         = aws_acm_certificate.eks_domain_cert.arn
  validation_record_fqdns = [for record in aws_route53_record.domain_cert_validation : record.fqdn]
}

# cria conta de servi√ßo lb Ingress Controller
resource "kubernetes_service_account" "lb_controller" {
  metadata {
    name      = var.load_balancer_name
    namespace = "kube-system"

    labels = {
      "app.kubernetes.io/component" = "controller"
      "app.kubernetes.io/name"      = var.load_balancer_name
    }

    annotations = {
      "eks.amazonaws.com/role-arn" = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${var.ingress_gateway_iam_role}"
    }
  }
}

# implanta o Ingress Controller
# https://github.com/kubernetes-sigs/aws-load-balancer-controller/tree/main/helm/aws-load-balancer-controller
resource "helm_release" "ingress_gateway" {
  name       = var.alb_controller_chart_name
  chart      = var.alb_controller_chart_name
  repository = var.alb_controller_chart_repo
  version    = var.alb_controller_chart_version
  namespace  = "kube-system"

  set {
    name  = "clusterName"
    value = var.cluster_name
  }

  set {
    name  = "serviceAccount.name"
    value = kubernetes_service_account.lb_controller.metadata.0.name
  }

  set {
    name  = "serviceAccount.create"
    value = "false"
  }
}
```

Na defini√ß√£o acima, usamos um novo certificado SSL emitido pela AWS para fornecer HTTPS em nosso ALB para ser colocado na frente de nossos pods do Kubernetes. Tamb√©m definimos algumas anota√ß√µes exigidas pelo servi√ßo [AWS Load Balancer Controller](https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/).

‚ö†Ô∏è Observa√ß√£o: esta configura√ß√£o usa uma fonte de dados para buscar uma zona DNS hospedada no Route53 criada fora desta configura√ß√£o do Terraform. Se voc√™ ainda n√£o tiver um, poder√° alter√°-lo livremente para criar novos recursos de DNS.

### DNS externo

O pr√≥ximo componente a ser implantado √© o servi√ßo [DNS externo](https://github.com/kubernetes-sigs/external-dns) que ser√° respons√°vel por sincronizar nossos servi√ßos e entradas expostos do Kubernetes e gerenciar nossos registros do Route53.

external-dns.tf
```ssh
# deploy 'external-dns' service
# https://github.com/kubernetes-sigs/external-dns
resource "helm_release" "external_dns" {
  name       = var.external_dns_chart_name
  chart      = var.external_dns_chart_name
  repository = var.external_dns_chart_repo
  version    = var.external_dns_chart_version
  namespace  = "kube-system"

  dynamic "set" {
    for_each = var.external_dns_values

    content {
      name  = set.key
      value = set.value
      type  = "string"
    }
  }

  set {
    name  = "serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn"
    value = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${var.external_dns_iam_role}"
  }

  # far√° com que o ExternalDNS veja apenas as zonas hospedadas correspondentes ao dom√≠nio fornecido, omitir√° o processamento de todas as zonas hospedadas dispon√≠veis
  set {
    name  = "domainFilters"
    value = "{${var.dns_hosted_zone}}"
  }

  set {
    name  = "txtOwnerId"
    value = data.aws_route53_zone.base_domain.zone_id
  }
}
```

O Helm chart de comando de DNS externo do Kubernetes requer algumas anota√ß√µes para o novo [certificado ACM](https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html) gerado para fornecer conex√µes SSL e tamb√©m para criar/modificar/excluir registros no dom√≠nio base do Route53.

### Namespaces do Kubernetes (opcional)

Agora terminamos de criar os componentes obrigat√≥rios. Para manter nossa implanta√ß√£o limpa e separada, podemos definir alguns namespaces do Kubernetes para nos ajudar a ter melhor gerenciamento e visibilidade em nosso cluster.

namespaces.tf
```ssh
# Cria namespaces no EKS
resource "kubernetes_namespace" "eks_namespaces" {
  for_each = toset(var.namespaces)

  metadata {
    annotations = {
      name = each.key
    }
    name = each.key
  }
}
```

A parte final √© definir a vari√°vel necess√°ria para essas configura√ß√µes e seus valores.

variables.tf
```ssh
ariable "cluster_name" {
  type        = string
  description = "Nome do cluster EKS."
}
variable "spot_termination_handler_chart_name" {
  type        = string
  description = "Nome do Chart Helm do manipulador de termina√ß√£o do EKS Spot."
}
variable "spot_termination_handler_chart_repo" {
  type        = string
  description = "Nome do reposit√≥rio Helm do manipulador de encerramento do EKS Spot."
}
variable "spot_termination_handler_chart_version" {
  type        = string
  description = "Vers√£o do Chart Helm do manipulador de termina√ß√£o do EKS Spot."
}
variable "spot_termination_handler_chart_namespace" {
  type        = string
  description = "Namespace do Kubernetes para implantar o gr√°fico Helm do manipulador de termina√ß√£o do EKS Spot."
}

# cria algumas vari√°veis
variable "external_dns_iam_role" {
  type        = string
  description = "Nome da fun√ß√£o do IAM associado ao servi√ßo de DNS externo."
}
variable "external_dns_chart_name" {
  type        = string
  description = "Nome do gr√°fico associado ao servi√ßo de DNS externo."
}

variable "external_dns_chart_repo" {
  type        = string
  description = "Gr√°fico de reposit√≥rio associado ao servi√ßo de DNS externo."
}

variable "external_dns_chart_version" {
  type        = string
  description = "Chart do reposit√≥rio associado ao servi√ßo de DNS externo."
}

variable "external_dns_values" {
  type        = map(string)
  description = "Mapa de valores exigido pelo servi√ßo externo-dns."
}

variable "name_prefix" {
  type        = string
  description = "Prefixo a ser usado em cada nome de objeto de infraestrutura criado na AWS."
}
variable "admin_users" {
  type        = list(string)
  description = "Lista de administradores do Kubernetes."
}
variable "developer_users" {
  type        = list(string)
  description = "Lista de desenvolvedores do Kubernetes."
}

variable "dns_hosted_zone" {
  type        = string
  description = "Nome da zona DNS a ser usado do EKS Ingress."
}
variable "load_balancer_name" {
  type        = string
  description = "Nome do servi√ßo do balanceador de carga."
}
variable "alb_controller_iam_role" {
  type        = string
  description = "Nome da fun√ß√£o do IAM associado ao servi√ßo do balanceador de carga."
}
variable "alb_controller_chart_name" {
  type        = string
  description = "Nome do gr√°fico do Helm do AWS Load Balancer Controller."
}
variable "alb_controller_chart_repo" {
  type        = string
  description = "Nome do reposit√≥rio Helm do AWS Load Balancer Controller."
}
variable "alb_controller_chart_version" {
  type        = string
  description = "Vers√£o do gr√°fico Helm do AWS Load Balancer Controller."
}

# cria algumas vari√°veis
variable "namespaces" {
  type        = list(string)
  description = "Lista de namespaces a serem criados em nosso cluster EKS."
}
```

config-developement.tfvars
```ssh
spot_termination_handler_chart_name      = "aws-node-termination-handler"
spot_termination_handler_chart_repo      = "https://aws.github.io/eks-charts"
spot_termination_handler_chart_version   = "0.18.1"
spot_termination_handler_chart_namespace = "kube-system"


external_dns_iam_role      = "external-dns"
external_dns_chart_name    = "external-dns"
external_dns_chart_repo    = "https://kubernetes-sigs.github.io/external-dns/"
external_dns_chart_version = "1.9.0"

external_dns_values = {
  "image.repository"   = "k8s.gcr.io/external-dns/external-dns",
  "image.tag"          = "v0.11.0",
  "logLevel"           = "info",
  "logFormat"          = "json",
  "triggerLoopOnEvent" = "true",
  "interval"           = "5m",
  "policy"             = "sync",
  "sources"            = "{ingress}"
}

admin_users     = ["calvine-otieno", "jannet-kioko"]
developer_users = ["elvis-kariuki", "peter-donald"]

dns_hosted_zone = "calvineotieno.com"
load_balancer_name       = "aws-load-balancer-controller"
alb_controller_iam_role      = "load-balancer-controller"
alb_controller_chart_name    = "aws-load-balancer-controller"
alb_controller_chart_repo    = "https://aws.github.io/eks-charts"
alb_controller_chart_version = "1.4.1"

namespaces = ["dev"]
```

Por favor, altere o valor do dns_base_domain para um dom√≠nio que voc√™ possui e tem acesso.

‚ö†Ô∏èObserva√ß√£o: os usu√°rios do IAM exibidos acima n√£o s√£o reais. Voc√™ precisa alter√°-los/personaliz√°-los de acordo com os grupos de usu√°rios em sua conta da AWS. Esses nomes de usu√°rio n√£o precisam existir como identidades do AWS IAM no momento em que voc√™ est√° criando o cluster, pois eles ficar√£o apenas dentro do cluster do Kubernetes. Leia mais [aqui](https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html#create-kubeconfig-automatically) para saber como a AWS CLI lida com a correla√ß√£o de nome de usu√°rio do IAM/Kubernetes.

Agora que terminamos de criar todas as configura√ß√µes necess√°rias, vamos agrup√°-las em um m√≥dulo. Primeiro, precisamos criar a configura√ß√£o do provedor e o manifesto de dados.

versions.tf
```ssh
terraform {
  required_version = "1.1.9"

  backend "s3" {}

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "4.11.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "2.11.0"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "2.5.1"
    }
    time = {
      source  = "hashicorp/time"
      version = "0.7.2"
    }
  }
}


provider "aws" {
  region = "eu-west-1"
  default_tags {
    tags = {
      iac_environment = var.iac_environment_tag
    }
  }
}
```

data.tf
```ssh
data "aws_caller_identity" "current" {} # used for accesing Account ID and ARN
```

Veja como √© o m√≥dulo principal:

main.tf
```ssh
# m√≥dulo para criar o cluster
module "base" {
  source = "./base/"

  cluster_name            = var.cluster_name
  name_prefix             = var.name_prefix
  main_network_block      = var.main_network_block
  subnet_prefix_extension = var.subnet_prefix_extension
  zone_offset             = var.zone_offset
  eks_managed_node_groups = var.eks_managed_node_groups
  autoscaling_average_cpu = var.autoscaling_average_cpu
}

# m√≥dulo para configurar o cluster
module "config" {
  source = "./config/"

  cluster_name                             = module.base.cluster_id
  spot_termination_handler_chart_name      = var.spot_termination_handler_chart_name
  spot_termination_handler_chart_repo      = var.spot_termination_handler_chart_repo
  spot_termination_handler_chart_version   = var.spot_termination_handler_chart_version
  spot_termination_handler_chart_namespace = var.spot_termination_handler_chart_namespace
  dns_hosted_zone                          = var.dns_hosted_zone
  load_balancer_name                       = var.load_balancer_name
  alb_controller_iam_role                  = var.alb_controller_iam_role
  alb_controller_chart_name                = var.alb_controller_chart_name
  alb_controller_chart_repo                = var.alb_controller_chart_repo
  alb_controller_chart_version             = var.alb_controller_chart_version
  external_dns_iam_role                    = var.external_dns_iam_role
  external_dns_chart_name                  = var.external_dns_chart_name
  external_dns_chart_repo                  = var.external_dns_chart_repo
  external_dns_chart_version               = var.external_dns_chart_version
  external_dns_values                      = var.external_dns_values
  namespaces                               = var.namespaces
  name_prefix                              = var.name_prefix
  admin_users                              = var.admin_users
  developer_users                          = var.developer_users
}
```

E tamb√©m as vari√°veis necess√°rias:

variables.tf
```ssh
# for base/network.tf
variable "cluster_name" {
  type        = string
  description = "Nome do cluster EKS."
}
variable "iac_environment_tag" {
  type        = string
  description = "Tag da AWS para indicar o nome do ambiente de cada objeto de infraestrutura."
}
variable "name_prefix" {
  type        = string
  description = "Prefixo a ser usado em cada nome de objeto de infraestrutura criado na AWS."
}
variable "main_network_block" {
  type        = string
  description = "Bloco CIDR base a ser usado em nossa VPC."
}
variable "subnet_prefix_extension" {
  type        = number
  description = "Extens√£o de bits de bloco CIDR para calcular blocos CIDR de cada sub-rede."
}
variable "zone_offset" {
  type        = number
  description = "Deslocamento de extens√£o de bits de bloco CIDR para calcular sub-redes p√∫blicas, evitando colis√µes com sub-redes privadas."
}
variable "eks_managed_node_groups" {
  type        = map(any)
  description = "Mapa de defini√ß√µes de grupos de n√≥s gerenciados do EKS a serem criados."
}
variable "autoscaling_average_cpu" {
  type        = number
  description = "Limite m√©dio de CPU para dimensionar automaticamente inst√¢ncias do EKS EC2."
}
variable "spot_termination_handler_chart_name" {
  type        = string
  description = "Nome do Helm chart do manipulador de termina√ß√£o do EKS Spot."
}
variable "spot_termination_handler_chart_repo" {
  type        = string
  description = "Nome do reposit√≥rio Helm do manipulador de encerramento do EKS Spot."
}
variable "spot_termination_handler_chart_version" {
  type        = string
  description = "Vers√£o do gr√°fico Helm do manipulador de termina√ß√£o do EKS Spot."
}
variable "spot_termination_handler_chart_namespace" {
  type        = string
  description = "Namespace do Kubernetes para implantar o gr√°fico Helm do manipulador de termina√ß√£o do EKS Spot."
}
variable "dns_hosted_zone" {
  type        = string
  description = "Nome da zona DNS a ser usado do EKS Ingress."
}
variable "load_balancer_name" {
  type        = string
  description = "Nome do servi√ßo do balanceador de carga."
}
variable "alb_controller_iam_role" {
  type        = string
  description = "Nome da fun√ß√£o do IAM associado ao servi√ßo do balanceador de carga."
}
variable "alb_controller_chart_name" {
  type        = string
  description = "Nome do Helm chart do AWS Load Balancer Controller."
}
variable "alb_controller_chart_repo" {
  type        = string
  description = "Nome do reposit√≥rio Helm do AWS Load Balancer Controller."
}
variable "alb_controller_chart_version" {
  type        = string
  description = "Vers√£o do gr√°fico Helm do AWS Load Balancer Controller."
}
variable "external_dns_iam_role" {
  type        = string
  description = "Nome da fun√ß√£o do IAM associado ao servi√ßo de DNS externo."
}
variable "external_dns_chart_name" {
  type        = string
  description = "Nome do gr√°fico associado ao servi√ßo de DNS externo."
}
variable "external_dns_chart_repo" {
  type        = string
  description = "Chart de reposit√≥rio associado ao servi√ßo de DNS externo."
}
variable "external_dns_chart_version" {
  type        = string
  description = "Chart Repo associado ao servi√ßo de DNS externo."
}
variable "external_dns_values" {
  type        = map(string)
  description = "Mapa de valores exigido pelo servi√ßo externo-dns."
}
variable "namespaces" {
  type        = list(string)
  description = "Lista de namespaces a serem criados em nosso cluster EKS."
}
variable "admin_users" {
  type        = list(string)
  description = "Lista de administradores do Kubernetes."
}
variable "developer_users" {
  type        = list(string)
  description = "Lista de desenvolvedores do Kubernetes."
}
```

Agora estamos prontos para executar os comandos do terraform. Selecione a √°rea de trabalho.

Antes de executarmos o plano e aplicarmos os comandos, precisamos formatar e validar nossa configura√ß√£o para garantir que ela esteja em formato e estilo can√¥nicos e v√°lida.
```ssh
docker-compose -f docker-compose.yaml run --rm terraform fmt
docker-compose -f docker-compose.yaml run --rm terraform validate
```
Ap√≥s a confirma√ß√£o de que tudo est√° correto, agora vamos planejar e aplicar
```ssh
docker-compose -f docker-compose.yaml run --rm terraform plan -out=development.tfplan -var-file=base-development.tfvars -var-file=config-development.tfvars
docker-compose -f docker-compose.yaml run --rm terraform apply development.tfplan
```
Neste ponto, temos um cluster EKS de n√≠vel de produ√ß√£o. 

O cluster agora est√° pronto para hospedar aplicativos.

### √â isso por enquanto. 

Quando voc√™ terminar e n√£o quiser deixar o cluster EKS em execu√ß√£o, o que eu recomendo para evitar ser cobrado pela AWS, voc√™ pode primeiro acionar o trabalho de desinstala√ß√£o do HELM e, em seguida, acionar o trigger de destrui√ß√£o.